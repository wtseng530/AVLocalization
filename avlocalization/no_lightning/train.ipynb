{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3710jvsc74a57bd098b0a9b7b4eaaa670588a142fd0a9b87eaafe866f1db4228be72b4211d12040f",
   "display_name": "Python 3.7.10 64-bit ('base': conda)"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "source": [
    "from numpy.lib.function_base import average\n",
    "from torch.optim import optimizer\n",
    "import valeodata\n",
    "import laspy\n",
    "import logging\n",
    "import os\n",
    "import numpy as np\n",
    "import math\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import resnet_no_pool\n",
    "import torch.nn.functional as F\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython import display\n",
    "%matplotlib inline"
   ],
   "cell_type": "code",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_image(filename, im):\n",
    "\n",
    "    dpi = 100\n",
    "    fig = plt.figure(figsize=(im.shape[0]//dpi+1,im.shape[1]//dpi+1))\n",
    "    ax = plt.axes([0., 0., 1., 1.], frameon=False, xticks=[],yticks=[])\n",
    "    ax.imshow(im, interpolation='none')\n",
    "    plt.savefig(filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DFC2018_Im_Elevation_Dataset():\n",
    "\n",
    "    def __init__(self, iterations, target_scale=0.5, imsize=64):\n",
    "\n",
    "        image_scale = 0.05\n",
    "        points_scale = 1.0\n",
    "        target_scale = target_scale\n",
    "        self.iterations = iterations\n",
    "        self.imsize = imsize\n",
    "\n",
    "        rootdir = \"/datasets_local/DFC2018/test_mini_dataset\"\n",
    "        filename_points = os.path.join(rootdir, \"lidar_C3_271460_3289689.las\")\n",
    "        filename_image = os.path.join(rootdir, \"UH_NAD83_271460_3289689.tif\")\n",
    "\n",
    "        # load the image\n",
    "        im = Image.open(filename_image)\n",
    "        target_im_size = (int(im.width/(target_scale/image_scale)), int(im.height/(target_scale/image_scale)))\n",
    "        im = im.resize(target_im_size)\n",
    "        im = np.array(im)\n",
    "        im = im.transpose(1,0,2)\n",
    "        im = im[:,::-1]\n",
    "\n",
    "        im = im.astype(np.float32) / 255\n",
    "        self.im = torch.tensor(im, dtype=torch.float)\n",
    "\n",
    "        # load the points as an elevation map\n",
    "        with laspy.file.File(filename_points, mode=\"r\") as file:\n",
    "            points = np.stack([file.x, file.y, file.z], axis=1)\n",
    "\n",
    "        image_points = points * points_scale / target_scale\n",
    "        image_points_size = (math.ceil(image_points[:,0].max()-image_points[:,0].min()),\n",
    "                        math.ceil(image_points[:,1].max()-image_points[:,1].min()))\n",
    "\n",
    "\n",
    "        self.points = np.full(image_points_size, np.NaN)\n",
    "        pts_coords = (image_points[:,:2] - image_points[:,:2].min(axis=0)).astype(np.int)\n",
    "\n",
    "        elevation = image_points[:,2]\n",
    "        elevation = elevation - elevation.mean()\n",
    "        elevation_std = elevation.std()\n",
    "        elevation[elevation < -3*elevation_std] = np.NaN #float(\"nan\")\n",
    "        elevation[elevation > 3*elevation_std] = np.NaN #float(\"nan\")\n",
    "        elevation = (elevation-3*elevation_std) / (6*elevation_std) # resize [0,1]\n",
    "\n",
    "        self.points[pts_coords[:,0], pts_coords[:,1]] = elevation\n",
    "        self.points[np.isnan(self.points)] = -1\n",
    "        self.points = torch.tensor(self.points, dtype=torch.float)\n",
    "\n",
    "        self.target_size = image_points_size\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.iterations\n",
    "\n",
    "    def prepare_image_patch(self, im_patch):\n",
    "        im_patch = im_patch.permute(2,0,1)\n",
    "        return im_patch\n",
    "\n",
    "    def prepare_points_patch(self, points_patch):\n",
    "        points_patch = points_patch - points_patch.mean() # prevent from learning an elevation bias\n",
    "        return points_patch\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        \n",
    "        x = torch.randint(0, self.target_size[0]-self.imsize, (1,)).item()\n",
    "        y = torch.randint(0, self.target_size[1]-self.imsize, (1,)).item()\n",
    "\n",
    "        im_patch = self.im[x:x+self.imsize,y:y+self.imsize]\n",
    "        im_patch = self.prepare_image_patch(im_patch)\n",
    "\n",
    "\n",
    "        points_patch = self.points[x:x+self.imsize,y:y+self.imsize]\n",
    "        points_patch = points_patch.unsqueeze(0).expand_as(im_patch)\n",
    "\n",
    "        if torch.randint(2, (1,)).item():\n",
    "            im_patch = torch.flip(im_patch, dims=[1])\n",
    "        if torch.randint(2, (1,)).item():\n",
    "            im_patch = torch.flip(im_patch, dims=[2])\n",
    "        if torch.randint(2, (1,)).item():\n",
    "            points_patch = torch.flip(points_patch, dims=[1])\n",
    "        if torch.randint(2, (1,)).item():\n",
    "            points_patch = torch.flip(points_patch, dims=[2])\n",
    "\n",
    "        return im_patch, points_patch\n",
    "\n",
    "    def get_full_image(self):\n",
    "\n",
    "        im_patch = self.im\n",
    "        im_patch = self.prepare_image_patch(im_patch)\n",
    "        return im_patch\n",
    "\n",
    "    def get_full_points(self):\n",
    "        points_patch = self.points\n",
    "        points_patch = self.prepare_points_patch(points_patch)\n",
    "        return points_patch\n",
    "\n",
    "    def get_point_patch(self):\n",
    "        \n",
    "        x = torch.randint(0, self.target_size[0]-self.imsize, (1,)).item()\n",
    "        y = torch.randint(0, self.target_size[1]-self.imsize, (1,)).item()\n",
    "        points_patch = self.points[x:x+self.imsize,y:y+self.imsize]\n",
    "        points_patch = points_patch.unsqueeze(0).expand(3, -1, -1)\n",
    "        return points_patch, x, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def nt_xent_loss(out_1, out_2, temperature):\n",
    "    \"\"\"\n",
    "    Loss used in SimCLR\n",
    "    \"\"\"\n",
    "    \n",
    "    out_1 = F.normalize(out_1, dim=1)\n",
    "    out_2 = F.normalize(out_2, dim=1)\n",
    "\n",
    "    out = torch.cat([out_1, out_2], dim=0)\n",
    "    n_samples = len(out)\n",
    "\n",
    "    # Full similarity matrix\n",
    "    cov = torch.mm(out, out.t().contiguous())\n",
    "    sim = torch.exp(cov / temperature)\n",
    "\n",
    "    # Negative similarity\n",
    "    mask = ~torch.eye(n_samples, device=sim.device).bool()\n",
    "    neg = sim.masked_select(mask).view(n_samples, -1).sum(dim=-1)\n",
    "\n",
    "    # Positive similarity :\n",
    "    pos = torch.exp(torch.sum(out_1 * out_2, dim=-1) / temperature)\n",
    "    pos = torch.cat([pos, pos], dim=0)\n",
    "    loss = -torch.log(pos / neg).mean()\n",
    "\n",
    "    return loss, sim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "in_channels = 3\n",
    "out_channels = 128\n",
    "batch_size = 128\n",
    "num_workers = 8\n",
    "num_epochs = 10\n",
    "temperature = 1\n",
    "num_epoch_steps = 1000\n",
    "device = torch.device(\"cuda\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logging.info(\"Creating the dataset\")\n",
    "dataset = DFC2018_Im_Elevation_Dataset(iterations=num_epoch_steps*batch_size)\n",
    "dataloader = torch.utils.data.DataLoader(\n",
    "        dataset,\n",
    "        batch_size=batch_size,\n",
    "        shuffle=True,\n",
    "        num_workers=num_workers,\n",
    "    )\n",
    "\n",
    "logging.info(\"Creating the models\")\n",
    "net_image = resnet_no_pool.resnet18(num_classes=out_channels)\n",
    "net_image = net_image.to(device)\n",
    "net_points = resnet_no_pool.resnet18(num_classes=out_channels)\n",
    "net_points = net_points.to(device)\n",
    "net_proj = nn.Sequential(nn.Conv2d(out_channels, out_channels,1), nn.ReLU(), nn.Conv2d(out_channels, out_channels,1)).to(device)\n",
    "\n",
    "logging.info(\"Creating the optimizer\")\n",
    "optimizer = torch.optim.Adam(\n",
    "        [\n",
    "            {\n",
    "                \"params\": net_image.parameters(),\n",
    "                \"lr\": 0.001,\n",
    "            },\n",
    "            {\n",
    "                \"params\": net_points.parameters(),\n",
    "                \"lr\": 0.001,\n",
    "            },\n",
    "            {\n",
    "                \"params\": net_proj.parameters(),\n",
    "                \"lr\": 0.001,\n",
    "            }\n",
    "        ]\n",
    "    )\n",
    "\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "\n",
    "    total_loss = 0\n",
    "    average_accuracy_1 = 0\n",
    "    average_accuracy_2 = 0\n",
    "\n",
    "    t = tqdm(dataloader, ncols=100)\n",
    "    for batch_id, batch in enumerate(t):\n",
    "\n",
    "        images = batch[0].to(device)\n",
    "        points = batch[1].to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        image_features = net_image(images)\n",
    "        points_features = net_points(points)\n",
    "\n",
    "        image_features = F.adaptive_avg_pool2d(image_features, output_size=(1, 1))\n",
    "        points_features = F.adaptive_avg_pool2d(points_features, output_size=(1, 1))\n",
    "\n",
    "        image_proj = net_proj(image_features)\n",
    "        points_proj = net_proj(points_features)\n",
    "\n",
    "        image_proj = torch.flatten(image_proj, 1)\n",
    "        points_proj = torch.flatten(points_proj, 1)\n",
    "\n",
    "        loss, sim_mat = nt_xent_loss(image_proj, points_proj, temperature)\n",
    "\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        total_loss += loss.item()\n",
    "\n",
    "\n",
    "        sim_mat = sim_mat[image_proj.shape[0]:, :image_proj.shape[0]]\n",
    "        sim_mat = sim_mat.detach().cpu().numpy()\n",
    "\n",
    "        labels = np.arange(image_proj.shape[0])\n",
    "\n",
    "        predictions = np.argmax(sim_mat, axis=1)\n",
    "        accuracy = (predictions==labels).sum() / labels.shape[0]\n",
    "        average_accuracy_1 += accuracy\n",
    "\n",
    "        predictions = np.argmax(sim_mat, axis=0)\n",
    "        accuracy = (predictions==labels).sum() / labels.shape[0]\n",
    "        average_accuracy_2 += accuracy\n",
    "\n",
    "        t.set_description_str(f\"Loss {total_loss/(batch_id+1):.5e} - Acc1 {average_accuracy_1 / (batch_id+1) * 100:.2f}% - Acc2 {average_accuracy_2 / (batch_id+1) * 100:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# validation\n",
    "\n",
    "net_image.eval()\n",
    "net_points.eval()\n",
    "net_proj.eval()\n",
    "\n",
    "\n",
    "images = dataset.get_full_image()\n",
    "images = images.unsqueeze(0).to(device)\n",
    "\n",
    "\n",
    "with torch.no_grad():\n",
    "\n",
    "    image_features = net_image(images)\n",
    "    image_proj = net_proj(image_features)\n",
    "    image_proj = F.normalize(image_proj, dim=1)\n",
    "    image_proj = image_proj.permute(0, 2,3,1)\n",
    "    image_proj = image_proj.cpu()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "with torch.no_grad():\n",
    "    # image_proj = image_proj.squeeze(0)\n",
    "\n",
    "    points, x, y = dataset.get_point_patch()\n",
    "    points = points.unsqueeze(0).to(device)\n",
    "    points_features = net_points(points)\n",
    "    points_features = F.adaptive_avg_pool2d(points_features, output_size=(1, 1))\n",
    "    points_proj = net_proj(points_features)\n",
    "    points_proj = F.normalize(points_proj, dim=1)\n",
    "    points_proj = torch.flatten(points_proj, 1)\n",
    "    points_proj = points_proj.squeeze(0)\n",
    "    points_proj = points_proj.cpu()\n",
    "    \n",
    "    # compute simalirity \n",
    "    print(image_proj.shape)\n",
    "    print(points_proj.shape)\n",
    "\n",
    "    sim = torch.matmul(image_proj, points_proj.unsqueeze(1))\n",
    "    sim = sim.squeeze(3).squeeze(0)\n",
    "    sim = sim.cpu().numpy()\n",
    "    im = (images[0].cpu().permute(1,2,0) * 255).long().numpy()\n",
    "    im_pts = points[0,0].cpu().numpy()\n",
    "\n",
    "    im[x:x+64, y:y+64, 0] = 255\n",
    "    im[x:x+64, y:y+64, 1] = 0\n",
    "    im[x:x+64, y:y+64, 2] = 0\n",
    "\n",
    "    fig = plt.figure(figsize=(12,4), dpi= 100)\n",
    "    ax1 = plt.subplot(1, 3, 1)\n",
    "    ax2 = plt.subplot(1, 3, 2)\n",
    "    ax3 = plt.subplot(1, 3, 3)\n",
    "    ax1.imshow(im_pts)\n",
    "    ax2.imshow(im)\n",
    "    pos = ax3.imshow(sim)\n",
    "    fig.colorbar(pos, ax=ax3)\n"
   ]
  }
 ]
}